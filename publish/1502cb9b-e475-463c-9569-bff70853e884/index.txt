1:"$Sreact.fragment"
2:I[8708,["315","static/chunks/315-19ca0205aa2fdd55.js","458","static/chunks/458-4db2abcd82f4c9c6.js","124","static/chunks/124-8b6d8ff5759d10e8.js","893","static/chunks/893-b9a66611845054ef.js","177","static/chunks/app/layout-95b7119291bad279.js"],"Provider"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[32176,["315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","335","static/chunks/app/%5Bslug%5D/error-a802457068253cdc.js"],"default"]
6:I[6874,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],""]
7:I[38567,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Button"]
9:I[59665,[],"OutletBoundary"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/kouchou-ai/_next/static/css/a63443551c7d7d9f.css","style"]
0:{"P":null,"b":"qk6YNIbxUjunn1HP5nLnW","p":"/kouchou-ai","c":["","1502cb9b-e475-463c-9569-bff70853e884",""],"i":false,"f":[[["",{"children":[["slug","1502cb9b-e475-463c-9569-bff70853e884","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/kouchou-ai/_next/static/css/a63443551c7d7d9f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"suppressHydrationWarning":true,"lang":"ja","children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&display=swap","rel":"stylesheet"}],["$","link",null,{"rel":"icon","href":"/kouchou-ai/meta/icon.png","sizes":"any"}],false]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":[["slug","1502cb9b-e475-463c-9569-bff70853e884","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","p",null,{"children":"ページが見つかりませんでした"}],["$","$L6",null,{"href":"/","children":["$","$L7",null,{"children":"トップに戻る"}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8","$undefined",null,["$","$L9",null,{"children":["$La","$Lb",null]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","v3LNOl-Ztg6duFaAGisoe",{"children":[["$","$Lc",null,{"children":"$Ld"}],null]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
11:I[73811,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Header"]
12:I[81068,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Box"]
13:I[17921,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Heading"]
14:I[90310,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Text"]
15:I[7684,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Icon"]
16:I[4618,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"ClientContainer"]
22:I[91925,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Analysis"]
23:I[91548,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Separator"]
25:I[18607,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"Footer"]
17:T19e9,import concurrent.futures
import json
import logging
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.category_classification import classify_args
from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    classification_categories = config["extraction"]["categories"]
    if classification_categories:
        results = classify_args(results, config, workers)

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.ERROR)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
18:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
19:T1bf8,import json
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
1a:T333d,import json
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
1b:Ta48,"""Create summaries for the clusters."""

import json
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
1c:T3f38,"""Generate a convenient JSON output file."""

import json
from collections import defaultdict
from pathlib import Path
from typing import Any, TypedDict

import numpy as np
import pandas as pd

ROOT_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = ROOT_DIR / "scatter" / "pipeline" / "configs"
PIPELINE_DIR = ROOT_DIR / "broadlistening" / "pipeline"


def json_serialize_numpy(obj: Any) -> Any:
    """
    Recursively convert NumPy data types to native Python types for JSON serialization.

    Args:
        obj: Any Python object which might contain NumPy data types

    Returns:
        The same object structure with NumPy types converted to Python native types
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: json_serialize_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [json_serialize_numpy(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(json_serialize_numpy(item) for item in obj)
    else:
        return obj


class Argument(TypedDict):
    arg_id: str
    argument: str
    comment_id: str
    x: float
    y: float
    p: float
    cluster_ids: list[str]
    attributes: dict[str, str] | None
    url: str | None


class Cluster(TypedDict):
    level: int
    id: str
    label: str
    takeaway: str
    value: int
    parent: str
    density_rank_percentile: float | None


def hierarchical_aggregation(config) -> bool:
    try:
        path = f"outputs/{config['output_dir']}/hierarchical_result.json"
        results = {
            "arguments": [],
            "clusters": [],
            "comments": {},
            "propertyMap": {},
            "translations": {},
            "overview": "",
            "config": config,
        }

        arguments = pd.read_csv(f"outputs/{config['output_dir']}/args.csv")
        arguments.set_index("arg-id", inplace=True)
        arg_num = len(arguments)
        relation_df = pd.read_csv(f"outputs/{config['output_dir']}/relations.csv")
        comments = pd.read_csv(f"inputs/{config['input']}.csv")
        clusters = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")
        labels = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_merge_labels.csv")

        hidden_properties_map: dict[str, list[str]] = config["hierarchical_aggregation"]["hidden_properties"]

        results["arguments"] = _build_arguments(clusters, comments, relation_df, config)
        results["clusters"] = _build_cluster_value(labels, arg_num)

        # results["comments"] = _build_comments_value(
        #     comments, arguments, hidden_properties_map
        # )
        results["comment_num"] = len(comments)
        results["translations"] = _build_translations(config)
        # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの
        results["propertyMap"] = _build_property_map(arguments, comments, hidden_properties_map, config)

        with open(f"outputs/{config['output_dir']}/hierarchical_overview.txt") as f:
            overview = f.read()
        print("overview")
        print(overview)
        results["overview"] = overview

        # Convert non-serializable NumPy types to native Python types
        results = json_serialize_numpy(results)

        with open(path, "w") as file:
            json.dump(results, file, indent=2, ensure_ascii=False)
        # TODO: サンプリングロジックを実装したいが、現状は全件抽出
        create_custom_intro(config)
        if config["is_pubcom"]:
            add_original_comments(labels, arguments, relation_df, clusters, config)
        return True
    except Exception as e:
        print("error")
        print(e)
        return False


def create_custom_intro(config):
    dataset = config["output_dir"]
    args_path = PIPELINE_DIR / f"outputs/{dataset}/args.csv"
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    result_path = PIPELINE_DIR / f"outputs/{dataset}/hierarchical_result.json"

    input_count = len(comments)
    args_count = len(pd.read_csv(args_path))
    processed_num = min(input_count, config["extraction"]["limit"])

    print(f"Input count: {input_count}")
    print(f"Args count: {args_count}")

    # LLMプロバイダーとモデル名の判定
    def get_llm_provider_display():
        # configからプロバイダー情報を取得（優先）
        provider = config.get("provider", "openai")
        model = config.get("model", "unknown")

        # プロバイダー名をマッピング
        provider_names = {
            "openai": "OpenAI API",
            "azure": "Azure OpenAI API",
            "openrouter": "OpenRouter API",
            "local": "Local LLM",
        }

        provider_name = provider_names.get(provider, f"{provider} API")
        return f"{provider_name} ({model})"

    llm_provider = get_llm_provider_display()

    base_custom_intro = """{intro}
分析対象となったデータの件数は{processed_num}件で、これらのデータに対して{llm_provider}を用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。
"""

    intro = config["intro"]
    custom_intro = base_custom_intro.format(
        intro=intro, processed_num=processed_num, args_count=args_count, llm_provider=llm_provider
    )

    with open(result_path) as f:
        result = json.load(f)
    result["config"]["intro"] = custom_intro
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)


def add_original_comments(labels, arguments, relation_df, clusters, config):
    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出
    labels_lv1 = labels[labels["level"] == 1][["id", "label"]].rename(
        columns={"id": "cluster-level-1-id", "label": "category_label"}
    )

    # arguments と clusters をマージ（カテゴリ情報付与）
    merged = arguments.merge(clusters[["arg-id", "cluster-level-1-id"]], on="arg-id").merge(
        labels_lv1, on="cluster-level-1-id", how="left"
    )

    # relation_df と結合
    merged = merged.merge(relation_df, on="arg-id", how="left")

    # 元コメント取得
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    comments["comment-id"] = comments["comment-id"].astype(str)
    merged["comment-id"] = merged["comment-id"].astype(str)

    # 元コメント本文などとマージ
    final_df = merged.merge(comments, on="comment-id", how="left")

    # 必要カラムのみ整形
    final_cols = ["comment-id", "comment-body", "arg-id", "argument", "cluster-level-1-id", "category_label"]

    # 基本カラム
    for col in ["x", "y", "source", "url"]:
        if col in comments.columns:
            final_cols.append(col)

    # 属性カラムを追加
    attribute_columns = []
    for col in comments.columns:
        # attributeプレフィックスが付いたカラムを探す
        if col.startswith("attribute_"):
            attribute_columns.append(col)
            final_cols.append(col)

    print(f"属性カラム検出: {attribute_columns}")

    # 必要なカラムだけ選択
    final_df = final_df[final_cols]
    final_df = final_df.rename(
        columns={
            "cluster-level-1-id": "category_id",
            "category_label": "category",
            "arg-id": "arg_id",
            "argument": "argument",
            "comment-body": "original-comment",
        }
    )

    # 保存
    final_df.to_csv(PIPELINE_DIR / f"outputs/{config['output_dir']}/final_result_with_comments.csv", index=False)


def _build_arguments(
    clusters: pd.DataFrame, comments: pd.DataFrame, relation_df: pd.DataFrame, config: dict
) -> list[Argument]:
    """
    Build the arguments list including attribute information from original comments

    Args:
        clusters: DataFrame containing cluster information for each argument
        comments: DataFrame containing original comments with attribute columns
        relation_df: DataFrame relating arguments to original comments
        config: Configuration dictionary containing enable_source_link setting
    """
    cluster_columns = [col for col in clusters.columns if col.startswith("cluster-level-") and "id" in col]

    # Prepare for merging with original comments to get attributes
    comments_copy = comments.copy()
    comments_copy["comment-id"] = comments_copy["comment-id"].astype(str)

    # Get argument to comment mapping
    arg_comment_map = {}
    if "comment-id" in relation_df.columns:
        relation_df["comment-id"] = relation_df["comment-id"].astype(str)
        arg_comment_map = dict(zip(relation_df["arg-id"], relation_df["comment-id"], strict=False))

    # Find attribute columns in comments dataframe
    attribute_columns = [col for col in comments.columns if col.startswith("attribute_")]
    print(f"属性カラム検出: {attribute_columns}")

    arguments: list[Argument] = []
    for _, row in clusters.iterrows():
        cluster_ids = ["0"]
        for cluster_column in cluster_columns:
            cluster_ids.append(str(row[cluster_column]))  # Convert to string to ensure serializable

        # Create base argument
        argument: Argument = {
            "arg_id": str(row["arg-id"]),  # Convert to string to ensure serializable
            "argument": str(row["argument"]),
            "x": float(row["x"]),  # Convert to native float
            "y": float(row["y"]),  # Convert to native float
            "p": 0,  # NOTE: 一旦全部0でいれる
            "cluster_ids": cluster_ids,
            "attributes": None,
            "url": None,
        }

        # Add attributes and URL if available
        if row["arg-id"] in arg_comment_map:
            comment_id = arg_comment_map[row["arg-id"]]
            comment_rows = comments_copy[comments_copy["comment-id"] == comment_id]

            if not comment_rows.empty:
                comment_row = comment_rows.iloc[0]

                # Add URL if available and enabled
                if config.get("enable_source_link", False) and "url" in comment_row and comment_row["url"] is not None:
                    argument["url"] = str(comment_row["url"])

                # Add attributes if available
                if attribute_columns:
                    attributes = {}
                    for attr_col in attribute_columns:
                        # Remove "attribute_" prefix for cleaner attribute names
                        attr_name = attr_col[len("attribute_") :]
                        # Convert potential numpy types to Python native types
                        attr_value = comment_row.get(attr_col, None)
                        if attr_value is not None:
                            if isinstance(attr_value, np.integer):
                                attr_value = int(attr_value)
                            elif isinstance(attr_value, np.floating):
                                attr_value = float(attr_value)
                            elif isinstance(attr_value, np.ndarray):
                                attr_value = attr_value.tolist()
                        attributes[attr_name] = attr_value

                    # Only add non-empty attributes
                    if any(v is not None for v in attributes.values()):
                        argument["attributes"] = attributes

        arguments.append(argument)
    return arguments


def _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:
    results: list[Cluster] = [
        Cluster(
            level=0,
            id="0",
            label="全体",
            takeaway="",
            value=int(total_num),  # Convert to native int
            parent="",
            density_rank_percentile=0,
        )
    ]

    for _, melted_label in melted_labels.iterrows():
        # Convert potential NumPy types to native Python types
        level = (
            int(melted_label["level"]) if isinstance(melted_label["level"], int | np.integer) else melted_label["level"]
        )
        cluster_id = str(melted_label["id"])
        label = str(melted_label["label"])
        takeaway = str(melted_label["description"])
        value = (
            int(melted_label["value"]) if isinstance(melted_label["value"], int | np.integer) else melted_label["value"]
        )
        parent = str(melted_label.get("parent", "全体"))

        # Handle density_rank_percentile which might be None or a numeric value
        density_rank = melted_label.get("density_rank_percentile")
        if density_rank is not None:
            if isinstance(density_rank, float | np.floating):
                density_rank = float(density_rank)
            elif isinstance(density_rank, int | np.integer):
                density_rank = int(density_rank)

        cluster_value = Cluster(
            level=level,
            id=cluster_id,
            label=label,
            takeaway=takeaway,
            value=value,
            parent=parent,
            density_rank_percentile=density_rank,
        )
        results.append(cluster_value)
    return results


def _build_comments_value(
    comments: pd.DataFrame,
    arguments: pd.DataFrame,
    hidden_properties_map: dict[str, list[str]],
):
    comment_dict: dict[str, dict[str, str]] = {}
    useful_comment_ids = set(arguments["comment-id"].values)
    for _, row in comments.iterrows():
        id = row["comment-id"]
        if id in useful_comment_ids:
            res = {"comment": row["comment-body"]}
            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())
            if should_skip:
                continue
            comment_dict[str(id)] = res

    return comment_dict


def _build_translations(config):
    languages = list(config.get("translation", {}).get("languages", []))
    if len(languages) > 0:
        with open(PIPELINE_DIR / f"outputs/{config['output_dir']}/translations.json") as f:
            translations = f.read()
        return json.loads(translations)
    return {}


def _build_property_map(
    arguments: pd.DataFrame, comments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict
) -> dict[str, dict[str, str]]:
    property_columns = list(hidden_properties_map.keys()) + list(config["extraction"]["categories"].keys())
    property_map = defaultdict(dict)

    # 指定された property_columns が arguments に存在するかチェック
    missing_cols = [col for col in property_columns if col not in arguments.columns]
    if missing_cols:
        raise ValueError(
            f"指定されたカラム {missing_cols} が args.csv に存在しません。"
            "設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。"
        )

    for prop in property_columns:
        for arg_id, row in arguments.iterrows():
            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする
            value = row[prop] if not pd.isna(row[prop]) else None

            # Convert NumPy types to Python native types
            if value is not None:
                if isinstance(value, np.integer):
                    value = int(value)
                elif isinstance(value, np.floating):
                    value = float(value)
                elif isinstance(value, np.ndarray):
                    value = value.tolist()
                else:
                    # Convert any other types to string to ensure serialization
                    try:
                        value = str(value)
                    except Exception as e:
                        print(f"Error converting value to string: {e}")
                        value = None

            # Make sure arg_id is string
            str_arg_id = str(arg_id)
            property_map[prop][str_arg_id] = value

    return property_map
1d:T19e9,import concurrent.futures
import json
import logging
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.category_classification import classify_args
from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    classification_categories = config["extraction"]["categories"]
    if classification_categories:
        results = classify_args(results, config, workers)

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.ERROR)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
1e:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
1f:T1bf8,import json
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
20:T333d,import json
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
21:Ta48,"""Create summaries for the clusters."""

import json
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
8:[["$","div",null,{"className":"container","children":[["$","$L11",null,{}],["$","$L12",null,{"mx":"auto","maxW":"750px","mb":8,"children":[["$","$L13",null,{"textAlign":"left","fontSize":"xl","mb":5,"children":"レポート"}],["$","$L13",null,{"as":"h2","size":"4xl","mb":2,"className":"headingColor","children":"【全員市長】20250712"}],["$","$L14",null,{"fontWeight":"bold","fontSize":"xl","mb":2,"children":[["$","$L15",null,{"mr":1,"children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-messages-square","children":[["$","path","p1xzt8",{"d":"M14 9a2 2 0 0 1-2 2H6l-4 4V4a2 2 0 0 1 2-2h8a2 2 0 0 1 2 2z"}],["$","path","1cx29u",{"d":"M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"}],"$undefined"]}]}],"196","件"]}],["$","p",null,{"children":"奈良市の持続可能な発展に向けた施策として、地域交通インフラの整備や観光振興、スポーツ・文化施設の充実が求められています。また、若者や家族の定住促進や市民参加を重視した行政改革も重要なテーマです。観光業の活性化を図るための具体的な施策が提案され、地域経済の強化が期待されています。これらの取り組みは、地域全体の活性化に寄与することが目指されています。"}]]}],["$","$L16",null,{"result":{"arguments":[{"arg_id":"Acsv-1_0","argument":"大和西大寺駅の高架化を絶対に実現してほしい","x":2.506129,"y":11.41376,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-2_0","argument":"ロートFをJリーグ規格に合ったスタジアムへアップデートすべき","x":4.179643,"y":11.97353,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-3_0","argument":"大和西大寺の市有地に新しい図書館を設置すべきである。","x":2.8238122,"y":11.498917,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-3_1","argument":"新しい図書館は地域住民の交流の場となり、学びや心地よさを感じられる複合施設であるべきである。","x":5.062991,"y":11.30038,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-3_2","argument":"図書館は近鉄奈良線沿線で身近に利用したいサービスとして重要であり、駅直結の立地が望ましい。","x":2.9373019,"y":10.829184,"p":0,"cluster_ids":["0","1_2","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-4_0","argument":"奈良市内の交通網は脆弱であり、幹線道路の拡充をスピーディに着手すべき","x":2.827151,"y":10.15082,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-4_1","argument":"駅前の駐車場が少なく、車から電車への乗り換えが不便である","x":2.9662764,"y":10.162602,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-4_2","argument":"若い世代に魅力的に見えるよう、賃貸マンションや商業施設の充実、学校施設の老朽化対策に力を入れるべき","x":4.944221,"y":10.220027,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-5_0","argument":"奈良市を発展させるためには観光業に頼ることが重要である。","x":3.6974277,"y":9.8801,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-5_1","argument":"奈良公園周辺の古い建造物だけではなく、大型レジャー施設の誘致が必要である。","x":3.7764134,"y":11.431459,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-5_2","argument":"ナガシマスパーランドのようなレジャー施設と商業施設が一体となった施設が奈良に必要である。","x":3.7347655,"y":11.556657,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-5_3","argument":"古代の遺跡と最新のレジャー施設を組み合わせることで、奈良独自の魅力が生まれる。","x":3.7178848,"y":11.517204,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-5_4","argument":"鴻池陸上競技場を大改修し、国際大会や音楽ライブが開催できるスタジアムにすべきである。","x":4.5571046,"y":11.729108,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-5_5","argument":"奈良クラブのJリーグカテゴリーが上がることで、スポーツを通じて奈良市を盛り上げることができる。","x":4.0239983,"y":9.937515,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-5_6","argument":"奈良ドリームランドの跡地にレジャー施設を誘致し、官民一体で利用すべきである。","x":3.7183523,"y":11.636471,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-5_7","argument":"奈良公園、奈良ドリームランド跡地、鴻池陸上競技場が徒歩1時間圏内にある観光地を作ることで、全ての人が楽しめる場所になる。","x":3.6825995,"y":11.491443,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-5_8","argument":"この構想には多くの課題があるが、実現すればワクワクするものである。","x":4.106076,"y":7.120672,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-5_9","argument":"仲川市長のリーダーシップによって、この構想が実現できると信じている。","x":4.21308,"y":6.727275,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-6_0","argument":"奈良市内では少子化により子供会が解散している地域があり、体験格差が心配である。","x":5.2763224,"y":9.941132,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-6_1","argument":"小学校の校区内で複数の町内会にまたがった子供会を作るべきである。","x":5.6412125,"y":10.118095,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-6_2","argument":"夏休み中に自由参加できるラジオ体操を小学校や公園で開催するべきである。","x":5.3583117,"y":10.859133,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-6_3","argument":"地域の神社の祭りにおいて、氏子地でなくとも参加できるように、学校を通じて祭りの案内や参加申し込みができるようにするべきである。","x":5.5712633,"y":10.318781,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-7_0","argument":"多選禁止条例を実行し、嘘偽りのない市政を遂行すべきである。","x":4.3843126,"y":6.456643,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-7_1","argument":"市長は独断や偏見を避け、市議会と対話する行政を実行すべきである。","x":4.185396,"y":6.2536755,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-7_2","argument":"クリーンセンターは市長の責任のもとでゼロベースで見直し、議会や地域の意見を尊重すべきである。","x":4.1932745,"y":6.314059,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-8_0","argument":"公明党は反対の為の反対ばかりで迷走している。","x":4.717196,"y":6.5939918,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-8_1","argument":"公明党が社会の役に立たないなら宗教活動に専念した方が良い。","x":4.6189017,"y":7.13488,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-9_0","argument":"全員市長制度に賛成である。","x":4.112236,"y":6.3775973,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-9_1","argument":"議会で市民の意見が勝手に代弁されることに嫌気がさしている。","x":4.2157087,"y":5.885794,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-9_2","argument":"議員や政党と知り合いでない人の声も聞いて欲しい。","x":4.0922947,"y":5.6528163,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-10_0","argument":"年金暮らしの高齢者の資産調査をしっかり行うべきである。","x":5.985383,"y":8.662204,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-10_1","argument":"資産があってもキャッシュフロー的に低所得者と認定されることは不公平である。","x":5.7449536,"y":8.383239,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-11_0","argument":"市会議員の数を半分の20人にするべきである。","x":3.917258,"y":5.8321505,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-11_1","argument":"20人の議会であれば多様な意見が十分反映できる。","x":3.9752376,"y":5.737194,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-12_0","argument":"街路樹が伸びすぎて汚いので、要らない木は伐採して花壇にすべき","x":2.9017444,"y":7.078651,"p":0,"cluster_ids":["0","1_5","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-12_1","argument":"街がきれいになり、選定費用が不要になる","x":2.9723244,"y":7.4763556,"p":0,"cluster_ids":["0","1_5","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-13_0","argument":"学校の給食に米飯の際は牛乳ではなくお茶にしてもらいたい","x":5.873649,"y":10.267321,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-13_1","argument":"牛乳代をおかず代に回して欲しい","x":6.0558205,"y":10.122504,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-13_2","argument":"家で牛乳をたくさん飲んでいるので、学校で飲ませる必要はないと思う","x":5.800233,"y":10.392937,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-14_0","argument":"奈良市に現代美術館を作るべきである。","x":3.7621267,"y":10.949829,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-14_1","argument":"現在の美術館は素人の発表会にしか使われておらず、もったいない。","x":5.0294695,"y":11.523962,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-14_2","argument":"市民の趣味の発表は公民館で行うべきで、もっと全国や世界から人々が訪れたくなるような企画を行う美術館が必要である。","x":4.914897,"y":11.667856,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-14_3","argument":"観光客の滞在時間が延びることで経済効果が大きくなり、投資費用も回収できるのではないか。","x":2.4871771,"y":9.063037,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-15_0","argument":"県庁を橿原に移転すべき","x":5.23522,"y":7.188444,"p":0,"cluster_ids":["0","1_5","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-15_1","argument":"県庁の跡地をホテルか美術館にすべき","x":4.5160513,"y":11.487835,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-15_2","argument":"市役所はたまに行く用事があるが、県庁には行ったことがない","x":5.2157273,"y":7.0889344,"p":0,"cluster_ids":["0","1_5","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-16_0","argument":"現在のロートフィールドをJ2基準の1万人の屋根付きスタジアムに整備すべき","x":4.2454095,"y":11.925278,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-16_1","argument":"地元民や観光客の更なる誘致を目指すべき","x":3.0430398,"y":9.394891,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-17_0","argument":"着物や和装をもっと振興して欲しい。","x":4.706308,"y":8.702767,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-18_0","argument":"大安寺の新駅には期待している","x":2.5391026,"y":11.399717,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-18_1","argument":"新駅はバリアフリーで先進的なまちづくりを目指すべき","x":2.5837214,"y":11.162689,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-19_0","argument":"マンションを作るために新しい電柱が歩道の真ん中に生えて困っている","x":2.8149297,"y":6.978405,"p":0,"cluster_ids":["0","1_5","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-19_1","argument":"市長ならその電柱を抜くべきだ","x":3.2381554,"y":6.557749,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-20_0","argument":"第一避難所のエアコン設置を早急に完了するべきである","x":3.138837,"y":11.121734,"p":0,"cluster_ids":["0","1_2","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-21_0","argument":"市役所や学校にソーラーパネルを設置すべきである。","x":4.780841,"y":11.106414,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-22_0","argument":"新駅周辺の企業誘致を進めるべきである。","x":3.153392,"y":9.30687,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-23_0","argument":"市街化調整区域を減らしてほしい。","x":3.153135,"y":7.278342,"p":0,"cluster_ids":["0","1_5","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-24_0","argument":"日本国籍以外の方の生活保護は許さないべきである。","x":5.816339,"y":8.149855,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-24_1","argument":"外国籍の方を受け入れるなら、税金を納めてルールを守る人だけを受け入れるべきである。","x":5.9359193,"y":8.200262,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-24_2","argument":"不法滞在は犯罪であり、迷惑である。","x":5.0836086,"y":7.6928306,"p":0,"cluster_ids":["0","1_5","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-24_3","argument":"日本人が海外で生活する際、現地のルールを守るのは当たり前であり、それが守れない外国籍の方はいらない。","x":5.9071164,"y":8.08371,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-25_0","argument":"奈良公園の鹿がごみを食べてしまわないようにゴミ箱を増設すべきである。","x":3.8367083,"y":10.620657,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-26_0","argument":"奈良公園の鹿のフンで発電するべきである。","x":3.6897912,"y":10.803755,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-27_0","argument":"週末に遊んでいる公用車を市民や観光客に貸すシェアサイクルを実施すべき","x":3.5079873,"y":8.865264,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-27_1","argument":"シェアサイクルは市の収入にもなり、駐車場のないマンションに住む人々が郊外に買い物に行く手助けになる","x":3.603864,"y":8.732659,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-28_0","argument":"水族館、動物園、植物園のいずれかが欲しい","x":4.3833075,"y":11.014765,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-28_1","argument":"民間運営にすれば、沢山の税金を使わなくても良いと思う","x":2.787685,"y":8.153421,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-29_0","argument":"平城京の無駄な駐車場を24時間の観光駐車場にすべき","x":2.7457256,"y":9.957312,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-29_1","argument":"無料のシャトルバスで東大寺や春日大社まで行けるようにすべき","x":2.256783,"y":10.98561,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-29_2","argument":"これにより渋滞が減り、滞在時間が延びるのではないか","x":2.5643537,"y":9.282222,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-30_0","argument":"保育士の処遇改善を増やすことで人材確保に繋がると思う","x":5.45124,"y":8.744456,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-31_0","argument":"経済発展に頼らない将来の発展が必要である。","x":4.5012803,"y":8.25549,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-32_0","argument":"資本主義とは一線を画した町が存在するべきである。","x":3.677319,"y":6.763776,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-33_0","argument":"大和西大寺から奈良公園までのワンコインバスを復活して欲しい","x":2.396318,"y":11.079579,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-34_0","argument":"画一的な計画都市ではなく、良い意味で混沌とした都市が望ましい","x":3.6073341,"y":6.789094,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-35_0","argument":"チェーン店が少ない町は、個人商店が活きるまちであるべき。","x":3.124202,"y":8.350949,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-36_0","argument":"市内宿泊3泊目に10,000円還元キャンペーンを実施すべき","x":2.913158,"y":9.028034,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-37_0","argument":"新人社会人に対して毎月5,000円の賃貸家賃補助キャンペーンを実施すべき","x":4.2032847,"y":8.739722,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-38_0","argument":"観光地をやめるべきである。","x":2.6996458,"y":9.361683,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-39_0","argument":"奈良駅前に各社の奈良漬試食コーナーを創出すべき","x":3.316469,"y":10.34123,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-40_0","argument":"大人も子どももクラブ活動を推進すべき","x":5.498913,"y":10.354059,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-40_1","argument":"体育館を大開放すべき","x":5.2634945,"y":11.403753,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-41_0","argument":"奈良プライドを醸成すべき","x":4.5006657,"y":9.963114,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-41_1","argument":"奈良のうたをみんなが歌える奈良市民を目指すべき","x":4.6978273,"y":10.033242,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-42_0","argument":"奈良市長杯の部別バレーボール大会は市職員向けのイベントである。","x":4.2530403,"y":9.73516,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-42_1","argument":"男女混合戦が行われる。","x":4.133925,"y":9.4242,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-43_0","argument":"奈良市職員向けに人事異動部長ドラフト制度を導入すべき","x":4.7268963,"y":9.445241,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-43_1","argument":"上位指名には給与を少し上乗せするべき","x":5.1368346,"y":8.717851,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-44_0","argument":"奈良市を物理的に明るくする運動が必要である","x":4.0164766,"y":10.19813,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-44_1","argument":"繁華街を明るくすることはワクワクする","x":3.336961,"y":8.427258,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-45_0","argument":"やすらぎの道にチンチン電車を導入すべき","x":2.5978303,"y":10.42757,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-45_1","argument":"南京終から城山台までチンチン電車で楽に移動できるようにすべき","x":2.5188503,"y":10.662073,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-46_0","argument":"奈良市のトイレはかなりキレイにするべきである。","x":3.5917373,"y":10.847569,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-46_1","argument":"裏が無いからオモテナシを大切にするべきである。","x":5.1391478,"y":8.076553,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-47_0","argument":"奈良市からアスリートを育成すべき","x":4.7266054,"y":9.936297,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-47_1","argument":"一条高校体育学部の重要性を強調すべき","x":5.5193777,"y":10.638712,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-48_0","argument":"道路の渋滞を無くすために、京奈和自動車道と第二阪奈道路を繋ぐべきである。","x":2.743168,"y":10.17914,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-49_0","argument":"八条の新駅にホテルやバスターミナルを設けるべきである","x":2.8235464,"y":11.225346,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-49_1","argument":"新駅を観光に出掛ける拠点にするべきである","x":2.6531768,"y":9.707018,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-50_0","argument":"奈良の立地を活かして工場誘致を進めるべきである。","x":3.8510034,"y":10.166364,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-50_1","argument":"北丿庄あたりの農地を転用するのが良いのではないか。","x":3.2173386,"y":7.911873,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-51_0","argument":"クリーンセンターはゴタゴタしているので、新斎苑の近くに移設すべきである。","x":3.3217025,"y":10.898052,"p":0,"cluster_ids":["0","1_2","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-52_0","argument":"若者を集めるために新しい大学を誘致すべきである。","x":5.1655216,"y":10.121748,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-53_0","argument":"八条大安寺の新駅には歓楽街を作るべきである","x":2.5651596,"y":11.4486,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-54_0","argument":"アクセスが良い公民館をリニューアルし、若者や学生が居場所として使える施設にすべき","x":5.143363,"y":11.17756,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-54_1","argument":"理想は生涯学習センターのような自習室が綺麗で使いやすい施設","x":5.2708225,"y":11.115684,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-54_2","argument":"整備の財源には企業版ふるさと納税を活用し、奈良市外の企業にもアプローチすべき","x":2.925393,"y":8.657692,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-54_3","argument":"公と民の館として、これからの時代に必要とされる施設にするべき","x":4.8093286,"y":11.554931,"p":0,"cluster_ids":["0","1_1","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-55_0","argument":"地域を超えたつながる活動が重要である。","x":4.3933673,"y":8.224371,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-56_0","argument":"仲川げんと心はひとつである。","x":4.6787724,"y":7.7117014,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-56_1","argument":"ガンバロー！！","x":4.1200294,"y":12.032577,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-57_0","argument":"フィルムコミッションを作るべきである。","x":3.4020977,"y":6.9106483,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-58_0","argument":"福祉に強いまちを目指すべきである。","x":5.1315374,"y":8.395459,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-59_0","argument":"人に優しいまちを目指すべき","x":4.952237,"y":8.492068,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-60_0","argument":"魚を養殖して売るべきである","x":4.0849576,"y":7.9950886,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-61_0","argument":"水道基本料金を無料にすべきである。","x":2.8840458,"y":7.691531,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-62_0","argument":"プレミアム商品券を発行すべきである。","x":4.50088,"y":8.078298,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-63_0","argument":"市民税を安くするべきである。","x":2.9807088,"y":7.731686,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-64_0","argument":"元ドリームランド跡地をスポーツランドにするべきである。","x":3.9940927,"y":11.80931,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-65_0","argument":"子供が遊べる公園が必要である","x":4.425578,"y":10.93486,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-65_1","argument":"公園には駐車場が必要である","x":4.0424957,"y":10.820728,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-66_0","argument":"鴻ノ池の競技場に電光掲示板を設置すべき","x":4.362997,"y":11.414061,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-67_0","argument":"子ども奈良CITYを続けたい","x":4.5651402,"y":10.108068,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-67_1","argument":"本当のまちの大人たちと高校生が集まるべき","x":5.3846436,"y":10.2785425,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-68_0","argument":"議員の数は人口に合わせた標準規模に適正化すべきである。","x":3.865335,"y":5.603809,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-68_1","argument":"議会報告会は本来の目的を果たすべきであり、学生ワークショップにすり替えるべきではない。","x":4.137493,"y":5.4865856,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-68_2","argument":"市議会はもっと有意義な形に転換する必要がある。","x":3.8088548,"y":6.038432,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-69_0","argument":"生産緑地の所有権移転時の納税猶予を緩和してほしい","x":3.129699,"y":7.903895,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-70_0","argument":"休日夜間救急で処方される薬の量を増やしてほしい","x":4.486697,"y":9.002186,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-70_1","argument":"1日分だけの処方では病院に行く負担が大きい","x":3.787452,"y":9.1930065,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-71_0","argument":"子どもセンターや児相の職員は教科書通りの対応をしているため、実際の子育ての現場を理解していない。","x":5.704738,"y":9.577476,"p":0,"cluster_ids":["0","1_3","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-71_1","argument":"子どもセンターや児相の職員は、虐待とは無縁の家庭で育ったため、現実的な視点が欠けている。","x":5.643432,"y":9.412691,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-72_0","argument":"市会議員を減らすべきであり、人数は35人までにするべきである。","x":3.7563763,"y":5.9617295,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-73_0","argument":"空き家を市場に流通させるために時限付きインセンティブを導入すべき","x":3.8875043,"y":8.190402,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-73_1","argument":"空き家対策の抜本的な見直しが必要であり、特に売却にかかる手間や費用の一部を負担するべき","x":4.0172963,"y":7.9698014,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-73_2","argument":"若い方や若い家族が奈良市内全域に住めるようにするための施策が必要","x":4.711622,"y":10.093889,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-73_3","argument":"インセンティブにかかる費用は税収や地域の活力として地域に還元されるべき","x":3.3964827,"y":7.967059,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-74_0","argument":"市内のでこぼこな道路の整備を早急に行うべき","x":2.4753466,"y":10.213812,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-74_1","argument":"奈良市内の道が悪いのは奈良県が管轄する県道が原因である","x":2.9008727,"y":10.245847,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-74_2","argument":"全国標準並みに道路を整備し、全国一移動が快適な道路を実現すべき","x":2.425955,"y":10.313223,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-74_3","argument":"奈良市は自動運転の実証実験に手を挙げるべき","x":3.3320625,"y":10.239334,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-74_4","argument":"テスラのロボタクシーは市民生活や駐車場のあり方を激変させるインパクトがある","x":3.3990612,"y":8.879761,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-74_5","argument":"関心が高い方の移住促進につながる","x":4.511159,"y":8.747893,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-74_6","argument":"奈良市では観光タクシーや過疎地の交通対策、流通コストの削減政策、CO2削減プランを国や県と連携して実施すべき","x":3.1128654,"y":9.716797,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-75_0","argument":"空き家を活用した学生向けシェアハウス事業は、低家賃で地域住民との交流を促進するべきである。","x":4.2285933,"y":8.695142,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-75_1","argument":"このシェアハウス事業は移住促進と地域コミュニティの強化に寄与する。","x":4.221768,"y":8.79218,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-76_0","argument":"ファミリーサポートセンターは良い仕組みである","x":5.58864,"y":9.023601,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-76_1","argument":"育児シェアをLINEアプリやコミュニティアプリと連携させ、市が安全面・保険面をサポートすべき","x":4.8760834,"y":9.0747,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-76_2","argument":"一緒に遊んだり、負担をシェアできるようになることが望ましい","x":4.0030465,"y":9.000377,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-77_0","argument":"市民提案アクションを形にしやすくする制度が必要である。","x":3.8261878,"y":6.3317933,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-77_1","argument":"自治会やコミュニティ単位での予算獲得型の可視化が重要である。","x":3.6022146,"y":6.4402566,"p":0,"cluster_ids":["0","1_5","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-77_2","argument":"地域で決める学校予算事業のような仕組みが有効である。","x":3.8707783,"y":7.489545,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-77_3","argument":"DAO型組織を活用して地域の小さな課題を柔軟に解消することが望ましい。","x":4.292412,"y":7.5520296,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-78_0","argument":"奈良市でも観光に特化した宿泊税を導入すべきである","x":2.893664,"y":9.208431,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-78_1","argument":"宿泊税の使用用途について観光事業者とのワークショップで意見を集約し、政治で決定する必要がある","x":2.5634973,"y":8.910506,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-78_2","argument":"宿泊税の徴収に賛同できる使用用途について議論を始めるべきである","x":2.5913327,"y":8.727162,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-79_0","argument":"自治連合会の区割りをあらためるべきである。","x":3.6512702,"y":6.0071073,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-79_1","argument":"都跡地区等、南北の差異が激しい。","x":3.5688558,"y":9.389993,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-80_0","argument":"クリーンセンターの早期実現を求めるべきである。","x":3.2572982,"y":11.010448,"p":0,"cluster_ids":["0","1_2","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-81_0","argument":"奈良公園を県から市に移管してほしい。","x":3.7814744,"y":10.491907,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-82_0","argument":"奈良公園にゴミ箱を設置して欲しい","x":3.7223074,"y":10.586395,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-83_0","argument":"障がい福祉の事業所を適正に監査すべき","x":5.9168315,"y":8.752753,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-83_1","argument":"ズボラな事業所が多すぎる","x":3.75098,"y":7.7073336,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-84_0","argument":"生活保護を厳しくしてほしい。","x":5.60993,"y":8.457057,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-84_1","argument":"現物支給を導入すべきである。","x":4.5161242,"y":8.295214,"p":0,"cluster_ids":["0","1_4","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-84_2","argument":"生活保護が働く意欲を失せさせる。","x":5.450823,"y":8.537788,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-85_0","argument":"外国人就労よりも、障がい者の働く場を増やすべきである。","x":6.002675,"y":8.56647,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-86_0","argument":"都市計画を見直すべき","x":3.2760682,"y":7.1393476,"p":0,"cluster_ids":["0","1_5","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-86_1","argument":"市街化を促進すべき","x":3.5435052,"y":8.257382,"p":0,"cluster_ids":["0","1_4","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-87_0","argument":"議会にもAIを導入して欲しい","x":3.9845212,"y":5.404824,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-88_0","argument":"議会中継をYouTubeで配信すべき","x":3.9077747,"y":5.427137,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-89_0","argument":"定例議会毎に第三者機関にて議員の通知簿をつけて公表すべき","x":4.0119314,"y":5.4823375,"p":0,"cluster_ids":["0","1_5","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-90_0","argument":"職員の給与をもっと上げるべき","x":5.223425,"y":8.737179,"p":0,"cluster_ids":["0","1_3","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-91_0","argument":"市は部長職や副市長に国の機関から出向させるべきである。","x":5.1507654,"y":7.0502357,"p":0,"cluster_ids":["0","1_5","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-91_1","argument":"出向先は総務省以外であるべきである。","x":5.3369823,"y":7.210222,"p":0,"cluster_ids":["0","1_5","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-92_0","argument":"部長級に、県から出向してほしい。","x":5.494844,"y":7.3658423,"p":0,"cluster_ids":["0","1_5","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-93_0","argument":"姉妹都市との交流を民間レベルで促進してほしい。","x":4.1145277,"y":8.913887,"p":0,"cluster_ids":["0","1_4","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-94_0","argument":"土地開発公社の復活が必要である","x":4.16966,"y":7.7946725,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-94_1","argument":"公共事業の土地取得をアウトソーシングすべきである","x":3.683308,"y":7.690996,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-95_0","argument":"中小企業の支店本店の固定資産税を減税すべき","x":2.8287382,"y":8.1830015,"p":0,"cluster_ids":["0","1_4","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-96_0","argument":"盆地魚の養殖は地域の水産業に貢献する可能性がある。","x":3.5556123,"y":8.02334,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-96_1","argument":"盆地魚の養殖は持続可能な食料供給の一環として重要である。","x":3.952452,"y":7.964119,"p":0,"cluster_ids":["0","1_4","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-97_0","argument":"喫煙スペースの拡充が必要である","x":4.6808367,"y":11.386684,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-98_0","argument":"外郭団体は解体すべきである。","x":4.5168138,"y":7.593263,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-99_0","argument":"近鉄奈良駅の県庁行き階段にエレベーターを設置すべき","x":2.9658,"y":10.641254,"p":0,"cluster_ids":["0","1_2","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-99_1","argument":"近鉄奈良駅の県庁行き階段にエスカレーターを設置すべき","x":2.7934084,"y":10.575748,"p":0,"cluster_ids":["0","1_2","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-100_0","argument":"おむつゴミを再生資源にする自治体があることは評価できる","x":3.5557032,"y":9.69833,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-100_1","argument":"高齢化に伴いおむつゴミが増えることが予想されるため、ゴミ減量策を早急に実施すべき","x":3.6799402,"y":10.00003,"p":0,"cluster_ids":["0","1_2","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-101_0","argument":"88億円まで貯まった貯金の一部を専用の基金にして、奈良の大学に通う学生の奨学金にするべき","x":4.9074345,"y":9.652653,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-101_1","argument":"卒業後に奈良市内で就職したら、返済を免除する制度を設けるべき","x":4.7721767,"y":9.52674,"p":0,"cluster_ids":["0","1_3","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-102_0","argument":"春日大社の入り口にゲートを設けて有料課金すべきである。","x":2.0698454,"y":10.79094,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-102_1","argument":"観光資源や観光インフラをタダ乗りしている観光客が多いのが問題である。","x":2.4198663,"y":9.117186,"p":0,"cluster_ids":["0","1_4","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-103_0","argument":"公民館などで高齢者向けのスマホ講習を徹底的に行うべき","x":6.1088395,"y":9.103848,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-103_1","argument":"高齢者や障害者はスマホの恩恵を受けるべきである","x":6.1556406,"y":8.859497,"p":0,"cluster_ids":["0","1_3","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-104_0","argument":"家族で遊べる公園がほしい","x":4.370718,"y":10.902168,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-104_1","argument":"大和郡山市九条公園のような公園が必要","x":4.1308265,"y":10.964673,"p":0,"cluster_ids":["0","1_1","2_17"],"attributes":null,"url":null}],"clusters":[{"level":0,"id":"0","label":"全体","takeaway":"","value":196,"parent":"","density_rank_percentile":0},{"level":1,"id":"1_2","label":"地域交通インフラの整備と観光振興による持続可能な発展","takeaway":"大和西大寺駅周辺の交通インフラの整備や新駅の設置を通じて、地域の利便性を向上させ、観光地へのアクセスを強化する具体的な施策が求められています。特に、無料シャトルバスやワンコインバスの復活、バリアフリーのまちづくり、道路の舗装改善や駐車場の増設など、地域住民や観光客にとっての利便性向上が重要視されています。また、奈良市の持続可能な発展に向けた観光業の振興や環境保護のための施策も提案されており、地域資源を活用した具体的なアクションが期待されています。","value":35,"parent":"0","density_rank_percentile":0.4},{"level":1,"id":"1_1","label":"地域のスポーツ・文化施設の整備と観光資源の活用による活性化","takeaway":"地域のスポーツ施設や文化施設の整備を通じて、国際イベントの誘致や地域住民の交流を促進し、観光資源を最大限に活用することが求められています。具体的には、鴻池陸上競技場の改修や元ドリームランド跡地の活用、現代美術館の設立などが提案されており、これにより地域の魅力を高め、観光客を呼び込むことが期待されています。また、公共施設の充実や多機能施設の整備により、地域住民が利用しやすい環境を整えることも重要なテーマとなっています。","value":30,"parent":"0","density_rank_percentile":0.2},{"level":1,"id":"1_3","label":"地域の未来を支える若者と家族のための施策強化","takeaway":"奈良市における若者や家族の定住促進を目指し、住環境の整備や教育機関の充実、地域連携による子育て支援の強化が求められています。また、福祉サービスの質向上や外国籍の方の受け入れに関する厳格な基準設定も重要な課題として挙げられています。地域全体での協力を通じて、若者や家族が安心して暮らせる環境を整えることが、地域の活性化や未来の創造に繋がると期待されています。","value":39,"parent":"0","density_rank_percentile":1},{"level":1,"id":"1_5","label":"市民参加と透明性を重視した地域行政の改革","takeaway":"仲川市長のリーダーシップのもと、市民の声を反映した透明性のある市政を実現するための取り組みが求められています。市議会の構造改革や市民参加の促進を通じて、議会の運営や市民の意見を尊重する姿勢が強調され、また市民が主体的に関与する予算獲得や多様性を重視した都市づくりが提案されています。さらに、都市環境の改善や地方行政の出向先の多様化に関する具体的な提案もあり、地域の未来に向けた期待と課題認識が共存しています。","value":40,"parent":"0","density_rank_percentile":0.8},{"level":1,"id":"1_4","label":"観光振興と地域経済活性化のための持続可能な施策","takeaway":"宿泊税の導入や地域資源の再利用を通じて観光業を振興し、地域経済の活性化を図るための具体的な施策が提案されています。観光客の滞在時間を延ばすことで経済効果を最大化し、地域の特性を活かした持続可能な開発を目指す姿勢が見受けられます。また、交通手段の改善や住民支援を通じて地域コミュニティの強化を図り、地域全体の活性化を促進するための多角的なアプローチが重要視されています。","value":52,"parent":"0","density_rank_percentile":0.6},{"level":2,"id":"2_0","label":"大和西大寺駅周辺の交通インフラと地域活性化の提案","takeaway":"この意見グループは、大和西大寺駅の高架化や新駅の設置を通じて、地域の交通インフラを整備し、観光地へのアクセス向上や地域活性化を図ることに関する具体的な提案が中心です。特に、無料シャトルバスやワンコインバスの復活、歓楽街やホテルの設置、バリアフリーのまちづくりなど、地域住民や観光客にとって利便性の高い施策が求められています。","value":10,"parent":"1_2","density_rank_percentile":0.84},{"level":2,"id":"2_21","label":"地域スポーツ施設の整備と国際イベント誘致","takeaway":"この意見グループは、地域のスポーツ施設やスタジアムの改修・整備に関する提案が中心であり、国際大会や音楽ライブの開催を可能にするための具体的な計画が示されています。また、地域のランドマークとしての役割を果たすために、元ドリームランド跡地の活用や、県庁跡地の新たな施設の提案も含まれています。","value":6,"parent":"1_1","density_rank_percentile":0.28},{"level":2,"id":"2_2","label":"地域住民の交流と学びを促進する多機能施設の必要性","takeaway":"この意見グループは、地域住民が利用できる自習室や美術館、図書館、体育館などの公共施設の整備に関するもので、特に利用しやすさや交流の場としての機能を重視しています。市民の趣味の発表や自由参加のイベントを通じて、地域の活性化や学びの場の提供が求められています。","value":8,"parent":"1_1","density_rank_percentile":0.44},{"level":2,"id":"2_20","label":"地域インフラ整備と公共サービスの向上","takeaway":"この意見グループは、地域のインフラや公共サービスの整備に関する要望が中心です。クリーンセンターや避難所のエアコン設置、エレベーターの設置、図書館の立地に関する意見が含まれており、地域住民の生活環境の向上を求める声が反映されています。","value":5,"parent":"1_2","density_rank_percentile":0.08},{"level":2,"id":"2_15","label":"奈良市内の交通インフラ整備と観光促進","takeaway":"この意見グループは、奈良市内の道路や交通インフラの整備に関する提案が中心であり、全国標準に合わせた道路整備や渋滞解消、観光地へのアクセス向上を目指しています。具体的には、道路の舗装改善、駐車場の増設、エスカレーターの設置、新駅の観光拠点化など、地域の交通網を強化し、観光客の利便性を高めることが求められています。","value":10,"parent":"1_2","density_rank_percentile":0.24},{"level":2,"id":"2_7","label":"若者と家族を支える奈良市の未来創造施策","takeaway":"この意見グループは、若い世代や家族が奈良市内に住みやすくするための施策やイベントの重要性を強調しています。具体的には、住環境の整備や教育機関の充実、アスリート育成、奨学金制度の導入など、若者の定住促進や地域活性化に向けた多角的なアプローチが求められています。また、地域のアイデンティティを高めるための文化活動やイベントも重要視されています。","value":10,"parent":"1_3","density_rank_percentile":0.6},{"level":2,"id":"2_10","label":"奈良市の持続可能な発展と観光振興に向けた提案","takeaway":"この意見グループは、奈良市の発展に向けた多様な提案が含まれており、自動運転の実証実験や観光業の振興、環境保護のためのゴミ対策、地域スポーツの活性化など、奈良市の持続可能な発展を目指す具体的な施策が中心です。特に、観光業の重要性や地域資源の活用に関する意見が多く見られ、地域の魅力を高めるための具体的なアクションが求められています。","value":10,"parent":"1_2","density_rank_percentile":0.92},{"level":2,"id":"2_6","label":"奈良の観光資源とレジャー施設の統合による地域活性化","takeaway":"この意見グループは、奈良の観光資源を活用し、レジャー施設や商業施設を統合することで、地域の魅力を高め、観光客を呼び込むことを目指しています。特に、奈良公園の鹿のフンを利用した発電や、現代美術館の設立、古代遺跡と最新のレジャー施設の融合など、地域の特性を生かした新しい観光地の創出が提案されています。","value":8,"parent":"1_1","density_rank_percentile":0.72},{"level":2,"id":"2_8","label":"仲川市長のリーダーシップによる市政改革と地域課題解決","takeaway":"この意見グループは、仲川市長のリーダーシップを信じ、透明性のある市政を実現するための多選禁止条例の実行や、DAO型組織を活用した地域課題の解決を提案しています。また、外郭団体の解体や公明党への批判も含まれ、より良い市政を目指す姿勢が強調されています。全体として、地域の未来に対する期待と課題認識が共存している点が特徴です。","value":8,"parent":"1_5","density_rank_percentile":1},{"level":2,"id":"2_12","label":"地域連携と子育て支援の強化","takeaway":"この意見グループは、地域内での子供会の設立や、学校給食の改善、クラブ活動の推進、地域イベントへの参加促進など、地域社会全体での子育て支援や子供の育成に関する提案が中心です。また、少子化による影響や体験格差の懸念も示されており、地域の大人と子供が協力し合う重要性が強調されています。","value":11,"parent":"1_3","density_rank_percentile":0.88},{"level":2,"id":"2_3","label":"市議会の構造改革と市民参加の促進","takeaway":"この意見グループは、市議会の議員数の適正化やAIの導入、市民の多様な意見を反映する仕組みの必要性に焦点を当てています。また、議会の運営や市民参加のあり方についても言及されており、議会報告会の目的の明確化や議会中継の配信など、透明性と市民の声を尊重する姿勢が求められています。","value":14,"parent":"1_5","density_rank_percentile":0.64},{"level":2,"id":"2_13","label":"市民参加型の予算獲得と多様性を重視した都市づくり","takeaway":"この意見グループは、自治会やコミュニティが予算を獲得しやすくするための可視化や、市民提案を実現する制度の必要性を強調しています。また、資本主義からの脱却や、画一的でない多様性のある都市の重要性についても言及されており、市民が主体的に関与することを重視した都市づくりの方向性が示されています。","value":7,"parent":"1_5","density_rank_percentile":0.68},{"level":2,"id":"2_14","label":"福祉サービスの現状と改善に向けた提言","takeaway":"この意見グループは、福祉サービスに関するさまざまな視点からの提言が集まっています。特に、子どもセンターや児相の職員の視点の欠如、高齢者や障がい者への支援の必要性、ファミリーサポートセンターの評価、そして障がい福祉事業所の監査の重要性が強調されています。また、高齢者向けのスマホ講習や資産調査の必要性についても言及されており、福祉サービスの質を向上させるための具体的なアクションが求められています。","value":7,"parent":"1_3","density_rank_percentile":0.76},{"level":2,"id":"2_4","label":"外国籍受け入れに関する厳格な条件設定","takeaway":"この意見グループは、外国籍の方を受け入れる際に、税金を納め、ルールを守ることを前提とする厳格な条件を求める声が中心です。また、日本人が海外でルールを守ることを前提に、同様の基準を外国籍の方にも適用すべきとの意見が強調されています。さらに、生活保護に関する不公平感や、資産と所得の関係についての懸念も含まれています。","value":4,"parent":"1_3","density_rank_percentile":0.04},{"level":2,"id":"2_23","label":"市街化調整区域の見直しと都市環境の改善","takeaway":"この意見グループは、市街化調整区域の削減や都市計画の見直しを通じて、街の美観や機能性を向上させることに焦点を当てています。具体的には、不要な街路樹の伐採や花壇の設置、マンション建設に伴うインフラの整備に関する懸念が表明されており、全体として都市環境の改善を求める声が集まっています。","value":5,"parent":"1_5","density_rank_percentile":0.16},{"level":2,"id":"2_24","label":"宿泊税導入による観光振興と経済効果の最大化","takeaway":"この意見グループは、宿泊税の導入を通じて観光資源の適正利用を促進し、観光客の滞在時間を延ばすことで地域経済に与えるポジティブな影響を強調しています。また、宿泊税の使用用途についての議論や観光事業者との連携の重要性も指摘されており、観光業界全体の持続可能な発展を目指す姿勢が見受けられます。","value":8,"parent":"1_4","density_rank_percentile":0.2},{"level":2,"id":"2_18","label":"地方行政における出向先の多様化と県庁の移転提案","takeaway":"この意見グループは、地方行政における出向先の選定に関する考え方や、県庁の移転に関する具体的な提案が中心です。出向先を総務省以外にすることや、部長職や副市長に国の機関からの出向を求める意見があり、また県庁の移転先として橿原を提案することで、地域の行政機能の向上を目指す姿勢が見受けられます。","value":6,"parent":"1_5","density_rank_percentile":0.12},{"level":2,"id":"2_5","label":"地域資源の再利用と観光・交通政策の強化","takeaway":"この意見グループは、自治体によるおむつゴミの再生資源化の評価から始まり、地域間の差異や交通対策、企業誘致、観光客の誘致に関する具体的な政策提案が含まれています。地域資源の有効活用と観光・交通の改善を通じて、地域の活性化を目指す意見が中心です。","value":5,"parent":"1_4","density_rank_percentile":0.56},{"level":2,"id":"2_22","label":"地域活性化と住民支援のための多角的施策","takeaway":"この意見グループは、育児シェアや地域コミュニティの強化を通じて、住民の安全や経済的支援を図る施策に関する提案が中心です。また、地域を超えたつながりや文化振興、経済発展に依存しない持続可能な発展を目指す姿勢が見受けられ、人に優しいまちづくりを重視する意見が多く含まれています。","value":8,"parent":"1_4","density_rank_percentile":0.96},{"level":2,"id":"2_17","label":"地域の公共施設と遊び場の充実","takeaway":"この意見グループは、市役所や学校へのソーラーパネル設置、家族や子供が遊べる公園の必要性、喫煙スペースの拡充、特定の公園の例を挙げた要望、駐車場の必要性、さらには水族館や動物園などのレクリエーション施設の要望が含まれています。これらは地域の公共施設や遊び場の充実を求める声として、一貫したテーマを持っています。","value":8,"parent":"1_1","density_rank_percentile":0.52},{"level":2,"id":"2_19","label":"市街化と地域活性化のための新しい交通手段の導入","takeaway":"この意見グループは、市街化を促進し、地域の活性化を図るために新しい交通手段やシェアリングエコノミーの導入が重要であるという視点に基づいています。テスラのロボタクシーやシェアサイクルの導入が市民生活や商業環境に与えるポジティブな影響、また企業版ふるさと納税を活用した財源確保の提案が含まれています。","value":7,"parent":"1_4","density_rank_percentile":0.8},{"level":2,"id":"2_9","label":"地域経済活性化のための税制改革とインセンティブ","takeaway":"この意見グループは、地域の経済を活性化させるために、税制の見直しや減税、インセンティブの導入を提案しています。具体的には、水道基本料金や市民税の無料化・減税、中小企業への固定資産税の軽減、農地の転用や生産緑地の納税猶予の緩和など、地域の活力を高めるための具体的な施策が中心となっています。","value":7,"parent":"1_4","density_rank_percentile":0.32},{"level":2,"id":"2_16","label":"福祉制度と職員処遇の改善に関する意見","takeaway":"この意見グループは、福祉制度のあり方や職員の処遇改善に関する多様な視点が集まっています。生活保護の厳格化や職員の給与向上、保育士の処遇改善を通じて、福祉サービスの質を向上させることが求められています。また、地域の福祉力を高めることへの期待も表れています。","value":7,"parent":"1_3","density_rank_percentile":0.48},{"level":2,"id":"2_11","label":"地域活性化と住民交流を促進する施策","takeaway":"この意見グループは、新人社会人や学生向けの賃貸家賃補助やシェアハウス事業を通じて、地域の移住促進やコミュニティの強化を目指す意見が中心です。また、休日夜間救急の医療サービスや姉妹都市との交流促進に関する意見も含まれており、地域住民との交流や負担の軽減を求める声が見受けられます。","value":8,"parent":"1_4","density_rank_percentile":0.4},{"level":2,"id":"2_1","label":"地域資源の活用と持続可能な開発の推進","takeaway":"この意見グループは、地域の特性を活かした学校予算事業や空き家対策、盆地魚の養殖など、地域資源を有効に活用し、持続可能な開発を目指すことに焦点を当てています。特に、空き家の流通促進や公共事業の効率化、地域の水産業の振興に関する具体的な提案が含まれており、地域経済の活性化を図るための多角的なアプローチが示されています。","value":9,"parent":"1_4","density_rank_percentile":0.36}],"comments":{},"propertyMap":{},"translations":{},"overview":"奈良市の持続可能な発展に向けた施策として、地域交通インフラの整備や観光振興、スポーツ・文化施設の充実が求められています。また、若者や家族の定住促進や市民参加を重視した行政改革も重要なテーマです。観光業の活性化を図るための具体的な施策が提案され、地域経済の強化が期待されています。これらの取り組みは、地域全体の活性化に寄与することが目指されています。","config":{"name":"1502cb9b-e475-463c-9569-bff70853e884","input":"1502cb9b-e475-463c-9569-bff70853e884","question":"【全員市長】20250712","intro":"仲川げんが掲げた全員市長プロジェクトで集まった声の分析\n分析対象となったデータの件数は104件で、これらのデータに対してOpenAI API (gpt-4o-mini)を用いて196件の意見（議論）を抽出し、クラスタリングを行った。\n","model":"gpt-4o-mini","provider":"openai","is_pubcom":true,"is_embedded_at_local":false,"local_llm_address":null,"extraction":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":104,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$17","model":"gpt-4o-mini"},"hierarchical_clustering":{"cluster_nums":[5,25],"source_code":"$18"},"hierarchical_initial_labelling":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$19","model":"gpt-4o-mini"},"hierarchical_merge_labelling":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$1a","model":"gpt-4o-mini"},"hierarchical_overview":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$1b","model":"gpt-4o-mini"},"hierarchical_aggregation":{"sampling_num":30,"hidden_properties":{},"source_code":"$1c"},"enable_source_link":false,"output_dir":"1502cb9b-e475-463c-9569-bff70853e884","skip-interaction":true,"without-html":true,"embedding":{"model":"text-embedding-3-small","source_code":"import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n    is_embedded_at_local = config[\"is_embedded_at_local\"]\n    # print(\"start embedding\")\n    # print(f\"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}\")\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model, is_embedded_at_local, config[\"provider\"])\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"},"hierarchical_visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_clustering","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_initial_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_merge_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_overview","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_aggregation","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_visualization","run":false,"reason":"skipping html output"}],"status":"running","start_time":"2025-07-11T23:24:23.240747","completed_jobs":[{"step":"extraction","completed":"2025-07-11T23:24:41.263096","duration":18.020748,"params":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":104,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$1d","model":"gpt-4o-mini"},"token_usage":62564},{"step":"embedding","completed":"2025-07-11T23:24:43.731883","duration":2.466893,"params":{"model":"text-embedding-3-small","source_code":"import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n    is_embedded_at_local = config[\"is_embedded_at_local\"]\n    # print(\"start embedding\")\n    # print(f\"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}\")\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model, is_embedded_at_local, config[\"provider\"])\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"},"token_usage":0},{"step":"hierarchical_clustering","completed":"2025-07-11T23:25:25.850047","duration":42.116289,"params":{"cluster_nums":[5,25],"source_code":"$1e"},"token_usage":0},{"step":"hierarchical_initial_labelling","completed":"2025-07-11T23:25:29.344648","duration":3.491091,"params":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$1f","model":"gpt-4o-mini"},"token_usage":21396},{"step":"hierarchical_merge_labelling","completed":"2025-07-11T23:25:32.836429","duration":3.48784,"params":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$20","model":"gpt-4o-mini"},"token_usage":11877},{"step":"hierarchical_overview","completed":"2025-07-11T23:25:34.712406","duration":1.872953,"params":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$21","model":"gpt-4o-mini"},"token_usage":1319}],"total_token_usage":97156,"token_usage_input":86945,"token_usage_output":10211,"lock_until":"2025-07-11T23:30:34.716236","current_job":"hierarchical_aggregation","current_job_started":"2025-07-11T23:25:34.716210","estimated_cost":0.01916835,"current_job_progress":null,"current_jop_tasks":null},"comment_num":104,"visibility":"public"}}],["$","$L22",null,{"result":"$8:0:props:children:2:props:result"}],["$","$L12",null,{"w":"fit-content","mx":"auto","children":["$","$L6",null,{"href":"/","children":["$","$L7",null,{"variant":"outline","size":"md","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],"一覧へ戻る"]}]}]}],["$","$L23",null,{"my":12,"maxW":"750px","mx":"auto"}],["$","$L12",null,{"maxW":"750px","mx":"auto","mb":24,"children":"$L24"}]]}],["$","$L25",null,{"meta":{"reporter":"仲川げん後援会","message":"","webLink":null,"privacyLink":null,"termsLink":null,"brandColor":"#2577B1","isDefault":true}}]]
b:null
f:[["$","title","0",{"children":"【全員市長】20250712 - 仲川げん後援会"}],["$","meta","1",{"name":"description","content":"奈良市の持続可能な発展に向けた施策として、地域交通インフラの整備や観光振興、スポーツ・文化施設の充実が求められています。また、若者や家族の定住促進や市民参加を重視した行政改革も重要なテーマです。観光業の活性化を図るための具体的な施策が提案され、地域経済の強化が期待されています。これらの取り組みは、地域全体の活性化に寄与することが目指されています。"}],["$","meta","2",{"property":"og:title","content":"【全員市長】20250712 - 仲川げん後援会"}],["$","meta","3",{"property":"og:description","content":"奈良市の持続可能な発展に向けた施策として、地域交通インフラの整備や観光振興、スポーツ・文化施設の充実が求められています。また、若者や家族の定住促進や市民参加を重視した行政改革も重要なテーマです。観光業の活性化を図るための具体的な施策が提案され、地域経済の強化が期待されています。これらの取り組みは、地域全体の活性化に寄与することが目指されています。"}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/1502cb9b-e475-463c-9569-bff70853e884/opengraph-image.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"【全員市長】20250712 - 仲川げん後援会"}],["$","meta","7",{"name":"twitter:description","content":"奈良市の持続可能な発展に向けた施策として、地域交通インフラの整備や観光振興、スポーツ・文化施設の充実が求められています。また、若者や家族の定住促進や市民参加を重視した行政改革も重要なテーマです。観光業の活性化を図るための具体的な施策が提案され、地域経済の強化が期待されています。これらの取り組みは、地域全体の活性化に寄与することが目指されています。"}],["$","meta","8",{"name":"twitter:image","content":"http://localhost:3000/1502cb9b-e475-463c-9569-bff70853e884/opengraph-image.png"}]]
26:I[81499,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","124","static/chunks/124-8b6d8ff5759d10e8.js","657","static/chunks/657-2381f903898ef60c.js","81","static/chunks/81-bd2635f93f5b2f66.js","15","static/chunks/15-070ae1c28165e1b1.js","609","static/chunks/609-c533c3eb03030555.js","182","static/chunks/app/%5Bslug%5D/page-1027ba582ddfc0a9.js"],"ReporterContent"]
24:["$","$L26",null,{"meta":"$8:1:props:meta","children":"$L27"}]
27:null
